# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2019-2021 Intel Corporation

---
# This file contains variables intended to be configured by user.
# It allows feature enabling and configuration.
# Per-host variables should be places in `inventory/default/host_vars` directory.
# Features should not be configured by changing roles' defaults (i.e. role/defaults/main.yml)

##################################################
##### User settings

### Proxy settings
proxy_env:
  # Proxy URLs to be used for HTTP, HTTPS and FTP
  # Comment out proxy settings except no_proxy while deploying verification_controller on AWS
  http_proxy: ""
  https_proxy: ""
  ftp_proxy: ""
  # No proxy setting contains addresses and networks that should not be accessed using proxy (e.g. local network, Kubernetes CNI networks)
  no_proxy: ""
  # all proxy need to use socks5 proxy to connect nats based servers(case with platform attestation components)
  all_proxy: ""

# Proxy for be used by GIT HTTP - required if GIT HTTP via proxy
git_http_proxy: "{{ proxy_env['http_proxy'] | default('') }}"

# Disable YUM plugins (e.g. Fastest Mirror)
os_remove_yum_plugins: true

##################################################

### Network Time Protocol (NTP)
# Enable machine's time synchronization with NTP server
ntp_enable: false
# Servers to be used by NTP instead of the default ones (e.g. 0.centos.pool.ntp.org)
ntp_servers: []

### Kernel, grub & tuned configuration
# Kernel, grub & tuned configurations are machine-type specific:
# - Edge Nodes - edit `inventory/default/group_vars/edgenode_group/10-default.yml`
# - Edge Controller - edit `inventory/default/group_vars/controller_group/10-default.yml`
# To provide configuration for specific machine, place the settings in `inventory/default/host_vars/_inventory_host_name.yml`

# Disable sriov kernel flags (intel_iommu=on iommu=pt)
iommu_enabled: true

# Enable hugepages
hugepages_enabled: false
# Size of a single hugepage (2M or 1G)
default_hugepage_size: 1G
# Amount of hugepages
hugepages_1G: 4
hugepages_2M: 0

# amount of memory "protected" from hugepages allocation in MB
mem_reserved: 1024

# Configure cpu_idle_driver
cpu_idle_driver_setup_enabled: false
cpu_idle_driver: poll

# isolcpus_enabled controls the CPU isolation mechanisms configured via grub command line.
isolcpus_enabled: false
# isolcpus is parameter for isolcpus, rcu_nocbs, nohz_full kernel command line arguments.
# For more information visit https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.htm
# This variable is required.
isolcpus: 2-4

# os_cpu_affinity_cpus pins the kthread and irq processing to selected cores using kthread_cpus and irqaffinity
# kernel command line arguments.
# For more information visit https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.htm
# Does nothing when empty.
os_cpu_affinity_cpus: ""

# Autogenerate isolated cores based on `cmk_exclusive_num_cores` when `cmk_enabled=true`.
autogenerate_isolcpus: false

# Enable additional grub parameters
additional_grub_parameters_enabled: false
additional_grub_parameters: ""

### Retry settings for network-based tasks
# - number_of_retries - configures how many times failed task should be retried
# - retry_delay - configures delay between retries (in seconds)
number_of_retries: 10
retry_delay: 5

# - allowed_number_of_k8s_retries - configures how many Kubernetes failures are allowed
allowed_number_of_k8s_retries: 10

### Network Edge's Kubernetes CNIs
# List of requested CNIs to be used in Network Edge mode
# CNIs are applied in order of definition
# Multus CNI is implicit - it will be applied as 2nd one if list contains more than 1
# Available CNIs:
# - calico (note: if the calico CNI is used, then it must be main/primary CNI, i.e. first on the list)
kubernetes_cnis:
  - calico

# Calico's CIDR (will be included automatically to noproxy when calico is included in kubernetes_cnis)
calico_cidr: "10.245.0.0/16"

## SmartEdge namespaces
system_namespace: "smartedge-system"
application_namespace: "smartedge-apps"

# SmartEdge installation directory
project_dir: "/opt/smartedge"

# SmartEdge users group
project_group: "{{ ansible_user }}"

# Rook ceph settings
rook_ceph_enabled: false
rook_ceph_namespace: "rook-ceph"

rook_ceph:
  mon_count: 1
  host_name: "{{ hostvars[groups['controller_group'][0]]['ansible_nodename'] }}"
  osds_per_device: "1"
  replica_pool_size: 1

# Kube-virt settings
kubevirt_enable: false
kubevirt_namespace: "kubevirt"
cdi_namespace: "cdi"

## SR-IOV Network Operator
sriov_network_operator_enable: true

## SR-IOV Network Operator configuration
sriov_network_operator_configure_enable: true

## SR-IOV Network Nic's Detection Application
sriov_network_detection_application_enable: false

# Nic names for the SR-IOV Network Operator
cvl_sriov_nics:
  RedHat:
    c0p0: "em3"
    c0p1: "em4"
    c1p0: "p5p1"
    c1p1: "p5p2"
  Debian:
    c0p0: "{{ 'enp137s0f0' if ansible_product_name == 'MOROCITY' else 'eno12399' }}"
    c0p1: "{{ 'enp137s0f1' if ansible_product_name == 'MOROCITY' else 'eno12409' }}"
    c1p0: "{{ 'enp137s0f2' if ansible_product_name == 'MOROCITY' else 'ens5f0' }}"
    c1p1: "{{ 'enp137s0f3' if ansible_product_name == 'MOROCITY' else 'ens5f1' }}"

# SriovNetworkNodePolicies list
sriov_network_node_policies:
  - name: "sriov-netdev-net-c0p0"
    resource_name: "sriov_netdev_net_c0p0"
    num_vfs: 4
    priority: 99
    vendor: 8086
    pf_names: ["{{ cvl_sriov_nics[ansible_os_family].c0p0 }}"]
    device_type: netdevice
  - name: "sriov-vfio-pci-net-c0p1"
    resource_name: "sriov_vfiopci_net_c0p1"
    num_vfs: 4
    priority: 99
    vendor: 8086
    pf_names: ["{{ cvl_sriov_nics[ansible_os_family].c0p1 }}"]
    device_type: vfio-pci
  - name: "sriov-netdev-net-c1p0"
    resource_name: "sriov_netdev_net_c1p0"
    num_vfs: 4
    priority: 99
    vendor: 8086
    pf_names: ["{{ cvl_sriov_nics[ansible_os_family].c1p0 }}"]
    device_type: netdevice
  - name: "sriov-vfio-pci-net-c1p1"
    resource_name: "sriov_vfiopci_net_c1p1"
    num_vfs: 4
    priority: 99
    vendor: 8086
    pf_names: ["{{ cvl_sriov_nics[ansible_os_family].c1p1 }}"]
    device_type: vfio-pci

# SriovNetworks list
sriov_networks:
  - name: sriov-netdev-network-c0p0
    network_namespace: "{{ application_namespace }}"
    ipam: |-
      {
      "type": "host-local",
      "subnet": "10.10.10.0/24",
      "rangeStart": "10.10.10.10",
      "rangeEnd": "10.10.10.41",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "10.10.10.1"
      }
    vlan: 0
    resource_name: sriov_netdev_net_c0p0
    link_state: enable
  - name: sriov-netdev-network-c1p0
    network_namespace: "{{ application_namespace }}"
    ipam: |-
      {
      "type": "host-local",
      "subnet": "10.10.20.0/24",
      "rangeStart": "10.10.20.10",
      "rangeEnd": "10.10.20.41",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "10.10.20.1"
      }
    vlan: 1
    resource_name: sriov_netdev_net_c1p0
    link_state: enable
  - name: sriov-vfio-network-c0p1
    network_namespace: "{{ application_namespace }}"
    ipam: |-
      {}
    resource_name: sriov_vfiopci_net_c0p1
    link_state: enable
  - name: sriov-vfio-network-c1p1
    network_namespace: "{{ application_namespace }}"
    ipam: |-
      {}
    resource_name: sriov_vfiopci_net_c1p1
    link_state: enable

## Enable OOT driver update for E810
e810_driver_enable: True

### Kubernetes Topology Manager configuration (for worker)
# CPU settings
cpu:
  # CPU policy - possible values: none (disabled), static (default)
  policy: "static"
  # Reserved CPUs for K8s and OS daemons - list of reserved CPUs
  reserved_cpus: "0,1"

# Kubernetes Topology Manager policy - possible values: none (disabled), best-effort (default), restricted, single-numa-node
topology_manager:
  policy: "best-effort"

## Network Edge Node Feature Discovery (NFD)
ne_nfd_enable: true

## Network Edge Helm Charts Storage Default Directory
ne_helm_charts_default_dir: "{{ project_dir }}/helm-charts"

###############
## Telemetry

telemetry_enable: true
# Telemetry flavor - possible values: common (default), flexran, smartcity, corenetwork
telemetry_flavor: common

# Telemetry namespace
telemetry_namespace: telemetry
# Node Exporter
telemetry_node_exporter_port: 9100
# Grafana
telemetry_grafana_enable: true

# Prometheus
telemetry_prometheus_scrape_interval_seconds: 60
telemetry_prometheus_retention: 15d
# A list of RemoteWriteSpec objects
# https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec
telemetry_prometheus_remote_write_targets: []
telemetry_prometheus_nodeport: 30000
# To expose the Prometheus on NodeIP, set this variable to True
telemetry_prometheus_nodeport_expose: false
telemetry_prometheus_tls_secret_name: prometheus-tls

# Prometheus-statsd-exporter
telemetry_statsd_exporter_enable: true
telemetry_statsd_exporter_udp_port: 8125
telemetry_statsd_exporter_tcp_port: 8125

# CAdvisor
telemetry_cadvisor_enable: true

# Telegraf
telemetry_telegraf_enable: false
telemetry_telegraf_port: 9105
###############

## Docker registry mirrors
## https://docs.docker.com/registry/recipes/mirror/
# docker_registry_mirrors:
#   - "https://docker-mirror.example.local"

## Docker insecure registries
## https://docs.docker.com/registry/insecure/
# docker_insecure_registries:
#   - "docker-insecure-mirror.example.local:5000"

## Hyperthreading
check_hyperthreading: false
expected_hyperthreading_state: "enabled"

## Persistent volumes root dir, according to system requirements (OF-5350) should be located inside /var/lib/*
_persistent_volumes_dest: "/var/lib/smartedge/"

## Enable cgroupfs to be used as a cgroup driver instead of systemd.
cgroupfs_enable: false

# rc.local path
_rc_local_path: "/etc/rc{{ '.local' if ansible_os_family == 'Debian' else '.d/rc.local' }}"

# Harbor timeout
harbor_timeout_min: 5

###############
## Platform attestation specific configurations

# Install isecl attestation components (TA, ihub, isecl k8s controller and scheduler extension)
platform_attestation_node: "{{ false if ansible_product_name == 'MOROCITY' else true }}"

# CMS hash from Intel-secl controller. Use following command:
# kubectl get secrets/cms-tls-cert-sha384 -n isecl --template={{.data.CMS_TLS_CERT_SHA384}} | base64 -d
isecl_cms_tls_hash: ""

# Host on which NFS server is setup. If left empty, NFS server will be installed on kubernetes controller
isecl_nfs_server: ""

# List of nfs clients allowed to server. Only used when NFS server installed on kubernetes controller
isecl_nfs_server_clients: []

# Host IP of node hosting isecl controlplane(core) services. This could be hosted on cloud as well.
isecl_control_plane_ip: ""

# Host IP of node hosting KMRA AppHSM service
kmra_apphsm_ip: ""

### Software Guard Extensions
# SGX requires kernel 5.11+, SGX enabled in BIOS and access to PCC service
sgx_enabled: "{{ false if ansible_product_name == 'MOROCITY' else true }}"

## Install HWE Kernel for SGX
install_hwe_kernel_enable: true

# PCCS server IP address
sgx_pccs_ip: ""

# PCCS server port address
sgx_pccs_port: "32666"

# To accept insecure HTTPS cert, set this option to FALSE
sgx_use_secure_cert: false

###############
## PCCS for SGX

# ApiKey - The PCCS uses this API key to request collaterals from Intel's Provisioning Certificate Service
pccs_api_key: ""

# PCCS client
pccs_user_password: ""

# PCCS administrator
pccs_admin_password: ""

# NodePort to access PCCS from outside of cluster
# pccs_access_port: 32666

# Enable userspace drivers installation
install_userspace_drivers_enable: false

# SRIOV-FEC operator
sriov_fec_operator_enable: false
sriov_fec_operator_configure_enable: false
